{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN15eoUGMd2kpL7WXj5Jx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burakemretetik/nanoGPT/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OdwARU4dQ630"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available and being used\")\n",
        "  print(\"Current device:\", torch.cuda.current_device())\n",
        "  print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "  print(\"GPU is not available or not being used\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYA5YNcYScpk",
        "outputId": "172d3f11-9bf2-4c36-95cb-b3e4c9acf242"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available and being used\n",
            "Current device: 0\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ],
      "metadata": {
        "id": "__09qdSWRBLt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/burakemretetik/nanoGPT/master/tiny-shakespeare.txt\n",
        "with open('tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1eFNTYTRFTk",
        "outputId": "bc327777-a496-413b-bb95-f616e7cf3b96"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 15:18:16--  https://raw.githubusercontent.com/burakemretetik/nanoGPT/master/tiny-shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘tiny-shakespeare.txt.1’\n",
            "\n",
            "tiny-shakespeare.tx 100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-16 15:18:17 (107 MB/s) - ‘tiny-shakespeare.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "oploW_QbROy8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "FTxLYV46Rn1U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the model\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_IzElbFRu6x",
        "outputId": "a6558608-506f-4ea6-ecf1-76e45193f7d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7600, val loss 1.9146\n",
            "step 1000: train loss 1.3903, val loss 1.5987\n",
            "step 1500: train loss 1.2644, val loss 1.5271\n",
            "step 2000: train loss 1.1835, val loss 1.4978\n",
            "step 2500: train loss 1.1233, val loss 1.4910\n",
            "step 3000: train loss 1.0718, val loss 1.4804\n",
            "step 3500: train loss 1.0179, val loss 1.5127\n",
            "step 4000: train loss 0.9604, val loss 1.5102\n",
            "step 4500: train loss 0.9125, val loss 1.5351\n",
            "step 4999: train loss 0.8589, val loss 1.5565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ60p3qcVxEC",
        "outputId": "23be7b29-b208-4564-cf63-eb216eae7614"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lord Angelo is well savereign thither in\n",
            "Hath on a rived of high death: if his soul\n",
            "Heaven fitly before your hot your process,\n",
            "Or in the view of spirit for your act,\n",
            "I often your bosoms, and you again unbeing honour\n",
            "Your for lifes, I'll die yours.\n",
            "\n",
            "KING EDWARD IV:\n",
            "STalfold, resolve the and Lad on't,\n",
            "Bost married again with tester commation,\n",
            "Now London the rotteness hath in least:\n",
            "This is a sea-covided worthy blood,\n",
            "Which made is my poor brother's life.\n",
            "\n",
            "LADY GREY:\n",
            "And, bright, my liege! Mistress Shore!\n",
            "\n",
            "KING EDWARD IV:\n",
            "Come, Peter's Somerset?\n",
            "\n",
            "LADWARD PHO:\n",
            "Why, so your high and hold quin will return.\n",
            "\n",
            "YORK:\n",
            "Marry of your saging, 'tis prisont duty!\n",
            "\n",
            "WARWICK:\n",
            "Where is that Tybatiste, and I hear for the Tower.\n",
            "\n",
            "KING HENRY VI:\n",
            "Now for my name now, good my lord, and pale not wit;\n",
            "In this monarch hath made you of kin,\n",
            "Which not hold mine own giver tears from you.\n",
            "The title is the deputy condition.\n",
            "\n",
            "MONTAGES:\n",
            "Now, Montague, if not the present to real,\n",
            "With assisters, I call him, and open as man.\n",
            "He could, verish crown, a wise as thou call'dst:\n",
            "Corneything that hangeth lived more than their camels;\n",
            "And what makes this a traitor  forfeits,\n",
            "Their means the vast frown do into their duty;\n",
            "No less their breath! Let them be the duke.\n",
            "\n",
            "LEONTES:\n",
            "Withink out men's cause. Is this world\n",
            "For 'tis well? Am thus more, or more, what the prince' doop\n",
            "Abroading them that thou hear my eldom,\n",
            "Canting this office which hath guiden sprope of us\n",
            "At reason? 'That's for horse this divince,\n",
            "He hath slack'd a s plention as thing it,\n",
            "If it be oft against your short,\n",
            "Your shonour leaves within observering your rascal,\n",
            "The life of death's soft wing, it will\n",
            "Compression what's yourself,\n",
            "The self of Margaret your father;\n",
            "I would is once and I. This sport, you know,\n",
            "You press'd, as he abid with you;\n",
            "Shall, my surery John, hath whispering York?\n",
            "\n",
            "NGROKE:\n",
            "That there any of this?\n",
            "\n",
            "BALTHASAR:\n",
            "The day nay, give me what think you to the great spring,\n",
            "It do but might you off, your father\n",
            "May was stand about your life: and so your please\n",
            "By sliddy, to punish your and most prayer:\n",
            "Now must I wink--Play from their help,--\n",
            "To stink they that enemy and more\n",
            "Concludes by the and other's measurer\n",
            "Of thy forein sleep\n",
            "Likes outwo bounds with Paul's flood.\n",
            "\n",
            "Nurse:\n",
            "What's good? ho! thy and there be haply entern'd?\n",
            "\n",
            "Nurse:\n",
            "Thy death, what comfort may over thy life?\n",
            "Thy pricknight resemblants by the catere!\n",
            "\n",
            "ROMEO:\n",
            "Look, why, ho! how have with a kiss me?\n",
            "\n",
            "RUTLAND:\n",
            "Pray your friend? O how never he come?\n",
            "\n",
            "MOPSA:\n",
            "Pray God, well, let's her all kind the mouth:\n",
            "To Martime is Bona, but proverhead\n",
            "To yield in his masquess. ha! I am so\n",
            "Strange here now me at Prince Elysi;\n",
            "A man which helps now my husband and heartens.\n",
            "Nay, in the king words this cunning tike two,\n",
            "And so wash upon this a melect,\n",
            "Of capt noice of the world's gain and buring them,--\n",
            "Any, since what they say'd just, and with\n",
            "My woman's majesty that husband in the night\n",
            "Of all the day of laments of smallines, hath\n",
            "A benefit-hateled, in very house, which I\n",
            "did left for the fortness or what sin. Yet, Master\n",
            "circutio's night what a vats good, which they\n",
            "have said, free she should aruste him. he, not you\n",
            "are the proclaimed, sir, in fair offices to\n",
            "honour in commity. he scarcks the noble princes\n",
            "hup fright his authority, and there was he goned, and\n",
            "himself you, to the people?\n",
            "\n",
            "Chonour:\n",
            "First, a daring brother, you may just; and go come but\n",
            "you the benefit of it.\n",
            "\n",
            "MISTRESS OVERDONE:\n",
            "I doubt, I do, then, with a stractine which the\n",
            "present a thing alone he hath his himself a\n",
            "content of this: if we are perditable good morrow and\n",
            "that he spoke which he chullunt on.\n",
            "\n",
            "GREMIO:\n",
            "There burn him together, and his course to guess.\n",
            "\n",
            "GREMIO:\n",
            "These time speopless at estimongs cre the prince younges\n",
            "close in your hopes.\n",
            "\n",
            "GREMIO:\n",
            "Masreasure of no mates, and not report.\n",
            "\n",
            "LUCENTIO:\n",
            "Her brother deprince than my love?\n",
            "Righs have I more no less; not you long bury; you,\n",
            "as young many shipber arrels, for his power is to leave\n",
            "The nobleness of yourself,\n",
            "Which, with an oath to but shake their sight.\n",
            "\n",
            "GLOUCESTER:\n",
            "That they have been more all things that in mightily.\n",
            "\n",
            "HASTINGS:\n",
            "My lord, I shall drive in the mockon, then.\n",
            "\n",
            "GLOUCESTER:\n",
            "What, then, I have known you to know it.\n",
            "\n",
            "CLARENCLIO:\n",
            "So, Signior Gloucester!\n",
            "\n",
            "WARWICK:\n",
            "Thine, my Lord, they will hear you: marry your man;\n",
            "Yea, and bid me, and that may again with us.\n",
            "\n",
            "RICHARD:\n",
            "Margary, hence; and else thy husband curse.\n",
            "\n",
            "GLOUCESTER:\n",
            "Not wasted you on her, marry, 'Sir, mine, wife.\n",
            "To where if you should off that; this sweet man may be\n",
            "sixt a ground and brandking by your brother together,\n",
            "I have an angry grantime to your heart\n",
            "And neither you and bit me? or, withal!\n",
            "\n",
            "Provost:\n",
            "Here's my man's to leave you tears to this grief:\n",
            "\n",
            "KING EDWARD IV:\n",
            "Dear brother, and wilt I cell him to London;\n",
            "Therefore was your mother and his name?\n",
            "\n",
            "SOMERSET:\n",
            "Ay, what, this, but he, fair coward! for the wolf?\n",
            "\n",
            "HASTINGS:\n",
            "Then!\n",
            "Banish hath; I, Gremio, the Tower fresh his life\n",
            "Forwards from this virtuous pleasure deserve\n",
            "In the whole defy on me;\n",
            "The tigestiler mutial; believe he to selt.\n",
            "\n",
            "GLOUCESTER:\n",
            "My haply Agazozins, the which doth three;\n",
            "Descend me your at fear: that think you have\n",
            "Make nok any thing footing could never\n",
            "Be\n",
            "Enfor your York.\n",
            "\n",
            "Lord:\n",
            "Bring how farewellent young Marcius?\n",
            "\n",
            "All:\n",
            "'Tis very not once, as a wock-a-a; and, let the print,\n",
            "With flower of ruits, a bootle, let's not you;\n",
            "Prove the air is instinct to he cravers\n",
            "With more than the to sun; be gentle use that short.\n",
            "\n",
            "SLY:\n",
            "What an oath you take? and this woman's life?\n",
            "A tyrant to him, a pent to his just,\n",
            "A fire in dear abilives; pray Saint Grace;\n",
            "I vid and nothing not now, but that wise his house\n",
            "Which in the most griever of this, I wish\n",
            "To command the throse most wildow the unasger.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Who cloud their woes tendary doing,\n",
            "And a dog, and a his brother beauty's flight,\n",
            "To bring his brows of his majesty.\n",
            "\n",
            "KING RICHARD II:\n",
            "Now with him, he is burstriction to his kind.\n",
            "And the only supposed when it he wants:\n",
            "Even and his side of France but one is right.\n",
            "\n",
            "YORK:\n",
            "Didst thou but conceive him by Henry live?\n",
            "Both why can yield yield his like and bird?\n",
            "O, in a puissance garden of his tables out\n",
            "Gaolen in Saint Lancaster, with grim,\n",
            "In my stepath, till I should broad thence thy breath\n",
            "sentence; with a many of loving thirts.\n",
            "This is unkit me for a to make thy jesty,\n",
            "Didst think my close presence which you pay to him,\n",
            "For my right were frown to him certion to the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "A widow of twain, let us swing it;\n",
            "If they never entail, know what with chance\n",
            "Master be stay's revenge; whereof, now my sworn?\n",
            "\n",
            "JOHN:\n",
            "Good--I boy, give your grace, give up with me\n",
            "To go you ther she favour,\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "There stands at once.\n",
            "\n",
            "BANCH:\n",
            "Then, yonder, uncle; why, not moneBay, for this.\n",
            "\n",
            "BISHMOND:\n",
            "Why, then give inquirest young Montague,\n",
            "A miserable absent worm, for we take't\n",
            "Till we know the Anthward protector?\n",
            "Is it thine elequal?\n",
            "\n",
            "HENRY PERCY:\n",
            "Then, Clarence, is it enough this left Calais?\n",
            "Even shortly right; my second withdraw crown'?\n",
            "Sty back, wept, reduly armour hither;\n",
            "For then envy, brace braftlyra loude going\n",
            "Bound up with innocent more back-Luffic\n",
            "Than proud death, for wonder wript\n",
            "Torms of all worldhings and battles; O wonder-for being\n",
            "May be the change fate? I warrant homacle,\n",
            "Why, more monthanvelly and that I do atch;\n",
            "My poor heavy sateth will account: if I do pite\n",
            "For that which yourself did left your heelone\n",
            "Lord Hastings, as forbid, your lies, shall make a up,\n",
            "To sworth in Camillo's! How holp you by\n",
            "sword some short! I'll ung horm: my lord, it is with it\n",
            "with that yoursty pointion; but if the occurse of\n",
            "your choice modeet prays for our offence, you shown\n",
            "your ungrace. Lords, you an hearing yourself, mark her, sir:\n",
            "known he neithertreants, or be necessible.\n",
            "\n",
            "ESCALUS:\n",
            "Besides, or being betrays shear than he take her than and\n",
            "she dead merril? Volscies being, and show it brave her.\n",
            "\n",
            "POMPEY:\n",
            "Look, I hope sour houself me, Master Sainter Sap xtol, and furnishe\n",
            "condemned with napthipable dew; so are a very great world, ho\n",
            "service, the which hath the done to know upo me: good\n",
            "ying with him, with both any heirs, horse pound a petty and\n",
            "part aloor nod her, which then verity roadine enter humour\n",
            "kissempt to his envy. Where is jewel\n",
            "to this, they are anoble lady?\n",
            "\n",
            "LEONTES:\n",
            "Do so not suffer.\n",
            "\n",
            "PAULINA:\n",
            "Truly?\n",
            "\n",
            "POLIXENES:\n",
            "One, a word: if they should not take they for\n",
            "Ceastive, hangled and killed. Shield,\n",
            "And for, there is the same therefore frame your springs,\n",
            "To have the vein fresh lift to good,\n",
            "Who had no do inspire little shrift,\n",
            "Than thoughts of his officer head wound, thus\n",
            "So 'tis cold the sea, and tread the unterth\n",
            "Hence, strid, to as inward the round,\n",
            "To the business of Marianena ascent's,\n",
            "You most counterfeit professes showing, a love man;\n",
            "One envast somethous all, that evides ack nothing\n",
            "So that frighther when they they hate more gone,\n",
            "So they not they wand whither.\n",
            "\n",
            "CLOFFORD:\n",
            "Fourth, like an integringes, they are took;\n",
            "And to but vengeance they have beseen them she,\n",
            "That's nothing that may move, Some men both,\n",
            "But they become to this gelt thither friar can\n",
            "And shall have compour of strable sdeed;\n",
            "And then stand will then be take thy chafe,\n",
            "Enforward is somewhat one old;\n",
            "It is in mortals, in hard-tilling smelter;\n",
            "Thinking facit, though sometime rich one,\n",
            "fright baits the enmies of the air\n",
            "And embrace.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Going hence; Bri fyond canst thou spoke else,\n",
            "Or, from this need the new prince may down.\n",
            "Good nature, bring me to think its a well?\n",
            "O, that all deserves, that season thing!\n",
            "Lord bows with the father was to my elforture!\n",
            "Oh, the evyce of Edward's happy I did;\n",
            "Thought me divided the duke in by glory daughter!\n",
            "I never was then so easy to sad,\n",
            "In France comes Gaunt, monduntle must be lian!\n",
            "Sweet triumphant make the fellow of Saint Christopham.\n",
            "I will not furtune hold; stand up his too ground:\n",
            "To him dispatch affection of the corown.\n",
            "\n",
            "KING RICHARD II:\n",
            "Ay, and lie with before Warwick's grief:\n",
            "To forward him to-morrow his grace to Bolingbroke.\n",
            "\n",
            "KING HE\n"
          ]
        }
      ]
    }
  ]
}